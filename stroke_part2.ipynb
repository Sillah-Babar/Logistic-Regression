{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48028d08",
   "metadata": {},
   "source": [
    "# COMP3160 Assignment 1\n",
    "\n",
    "## Part 2: Classification: Logistic regression - 50 marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092d1be3",
   "metadata": {},
   "source": [
    "This notebook contains the second task required for the first assignment in COMP3160. The focus of this notebook is in classification, so while some questions may look similar to the first notebook, please read the question carefully as the requirement can be quite different.\n",
    "\n",
    "This task contains 5 questions; the total marks available for this task is **50**. \n",
    "\n",
    "10% of the mark is awarded for well-structured and efficient code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "413be14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ccc2e7",
   "metadata": {},
   "source": [
    "### Task 1 -  Import CSV into pandas (5 Marks)\n",
    "\n",
    "1. Create a function to read the CSV file provided into a DataFrame. \n",
    "2. You MUST place the CSV file in the same directory/folder where your notebook is located. The method below should work without change when you give the file name \"stroke-data.csv\". \n",
    "4. The file imported has NA (not available) values in some columns. These rows need to be dropped as machine learning algorithms cannot process data with missing values. Remember when rows are dropped some (row) indexes will be missing. \n",
    "3. The first step in processing data is to review the data types of the features (columns). \n",
    "4. Use **pandas** features *columns* and *dtypes* to create a dictionary with column names as keys and the datatype as values.\n",
    "5. This function then returns the new dataframe (df) and the df_types dictionary (df_types), where a key-value pair represents column name-column's dtype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c197120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(fl):\n",
    "    \n",
    "    # Import the CSV file (fl)\n",
    "    # Your code goes here\n",
    "    df2=pd.read_csv(fl)\n",
    "    \n",
    "    # Drop all rows with NA values\n",
    "    # Your code goes here\n",
    "    print(df2.dtypes)\n",
    "    df2 = df2.dropna()\n",
    "\n",
    "    # Create a dictionary with keys the column names and values the type of data\n",
    "    # Your code goes here\n",
    "    df2_types = {}\n",
    "    cols=df2.columns\n",
    "\n",
    "    df2_types = {}\n",
    "    for i in range(len(cols)):\n",
    "        df2_types[cols[i]]=df2[cols[i]].dtypes\n",
    "    \n",
    "    return df2, df2_types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90968fc6",
   "metadata": {},
   "source": [
    "### Task 2  - Convert categorical (non-numeric) variables (20 marks)\n",
    "\n",
    "Many machine learning algorithms are designed to process numeric data and cannot natively handle categorical data. Therefore as part of the model building process, we must apply pre-processing steps to convert the data into an encoded format which the algorithms can handle.\n",
    "\n",
    "1. In the following function you will identify and convert categorical variables to numeric data type. \n",
    "2. You will need the python *dictionary* \"df2_types\" of the function \"process_data\" we created in task 1. We can use this to identify data in a categorical (non-numeric) data format.\n",
    "3. Create a list \"cat_ls\" of column names which are non-numeric. \n",
    "4. Process each column named in \"cat_ls\" separately. \n",
    "5. For a column name, say \"col_name\", find the *distinct* categories. For example, in column \"gender\" there are 2 categories \"Male\" and \"Female\". \n",
    "6. For a (categorical) column 'C' with *k* categories *k-1* new columns are created and 'C' is replaced by these new columns. For example, the \"*gender*\" column will be replaced by one numerical column. The column \"*smoking_status*\" is to be replaced with 2 numerical columns. This process is referred to as *one-hot encoding*.\n",
    "7. The encoding is done as follows. Suppose there are 3 categories \"cat1\", \"cat2\", \"cat3\" in column 'C'. Create 2 columns with distinct names, say \"cat_level1\", \"cat_level2. If an observation corresponding to a row is 'cat1' then put a 1 in 'cat_level1' and 0 in 'cat_level2' in the same row. If it is 'cat2' put 0 in 'cat_level1' and 1 in 'cat_level2' and put 0 in both if the observation is 'cat3'. \n",
    "8. It is simpler if the column has only 2 categories (like \"gender\"). It will be replaced by 1 column of 1's and 0's. \n",
    "9. The number of columns in the new DataFrame will be generally more than the original. For the *stroke-dataset* this number is 11. Remember to **drop** the old non-numeric columns.  \n",
    "10. Depending on how you do it the column orderings may change. This is important for identifying the output column \"stroke\". \n",
    "11. You may reorder the columns. Suggestion:move \"stroke\" to the last column in the new dataframe. \n",
    "13. You should NOT use any feature-processing modules from **sklearn** or pandas.get_dummies()for this part. If used the maximum mark for this task will not exceed 60%. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "201addf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numeric():\n",
    "    \n",
    "    # Read the appropriate file, should be in the same directory as the notebook\n",
    "    df, dict_types = process_data(\"Stroke_data.csv\")\n",
    "    cat_ls=[]\n",
    "    for key,value in dict_types.items():\n",
    "        print(value)\n",
    "        if value!=np.int64 and value!=np.float64:\n",
    "            cat_ls.append(key)\n",
    "\n",
    "    # Apply the one hot encoding process outlined to the new dataframe df2\n",
    "    # Your code goes here\n",
    "    df2 = None \n",
    "    df2=df\n",
    "    df2[\"sex\"]=0\n",
    "    df2[\"Marital_status\"]=0\n",
    "    df2[\"Smokes\"]=0\n",
    "    df2[\"NTSmokes\"]=0\n",
    "\n",
    "    for i, row in df2.iterrows():\n",
    "        if df2.at[i,\"gender\"]==\"Male\":\n",
    "            df2.at[i,\"sex\"]=1\n",
    "        else:\n",
    "            df2.at[i,\"sex\"]=0\n",
    "            \n",
    "        if df2.at[i,\"ever_married\"]==\"Yes\":\n",
    "            df2.at[i,\"Marital_status\"]=1\n",
    "        else:\n",
    "            df2.at[i,\"Marital_status\"]=0\n",
    "            \n",
    "        if df2.at[i,\"smoking_status\"]==\"never smoked\":\n",
    "            df2.at[i,\"NTSmokes\"]=1\n",
    "        elif df2.at[i,\"smoking_status\"]==\"smokes\":\n",
    "            df2.at[i,\"Smokes\"]=1\n",
    "        else:\n",
    "            df2.at[i,\"Smokes\"]=0\n",
    "            df2.at[i,\"NTSmokes\"]=0\n",
    "            \n",
    "    df2=df2.drop(['ever_married'], axis=1)\n",
    "    df2=df2.drop(['gender'], axis=1)\n",
    "    df2=df2.drop(['smoking_status'], axis=1)\n",
    "    #print(df2)\n",
    "   \n",
    "    col = df.pop(\"stroke\")\n",
    "     \n",
    "    #print(col.head(15))      \n",
    "    df2=df2.drop(['stroke'], axis=1)\n",
    "    df2[col.name]=col\n",
    "    #df2 = df2.insert(10, col.name, col)\n",
    "    #print(df2.head())\n",
    " \n",
    "    return df2      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79220aa5",
   "metadata": {},
   "source": [
    "### Task 3 - Generate ndarrays for train and test data (5 marks)\n",
    "\n",
    "1. Convert all columns except \"id\" and \"stroke\" into a numerical feature matrix **X**. The size of the matrix will be *no_of_rows* $\\times$  *(no_of_columns-2)*. The number of columns should be 9. \n",
    "2. Put the values in the \"stroke\" column in the array **y**. \n",
    "3. Use the sklearn [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method to generate *X_train, X_test, y_train. y_test*. \n",
    "5. In the *train_test_split()* method the fraction of data to be split for testing has to be specified. Vary this fraction between .2 to .33. Run your program  a few times to choose  an optimim value. The optimum will correspond to the fraction giving the best accuracy/precision (see Task 5). \n",
    "6. Return the 4 arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "47e157e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_arrays():\n",
    "    \n",
    "    # Call the function created in Task 2 to source the encoded data frame\n",
    "    df2 = convert_to_numeric()\n",
    "\n",
    "    # Create the X and y objects\n",
    "    # Your code goes here\n",
    "    X = None\n",
    "    y = None\n",
    "    \n",
    "    X = np.array(df2.iloc[:,1:10])\n",
    "    np.set_printoptions(suppress=True)\n",
    "    #the output vector\n",
    "    y = np.array(df2.iloc[:, 10])\n",
    "    \n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "    #print(X_train.shape)\n",
    "    #print(X_test.shape)\n",
    "    # Function returns the four newly created objects\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9abf4",
   "metadata": {},
   "source": [
    "### Task 4 - Create the logistic regression model (5 marks)\n",
    "1. In the following function you will use the [liner_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) from sklearn to create and train a logistic regression model. \n",
    "2. The model should be trained on the train set created in task 3. **Do not use the full dataset or test set for training**.\n",
    "2. As this is a binary classification problem (2 classes: \"stroke\", \"no-stroke\") the default model does not need significant adjustment\n",
    "3. You should refer to the document and experiment with changing the hyperparameters of the model\n",
    "\n",
    "\n",
    "Once you have a trained a model, answer the below questions:\n",
    "1. In the LogisticRegression class, the first keyword argument is *penalty='l2'*. What is penalized and why? Explain this in 2 sentences.  \n",
    "2. Instead of $l_2$ penalty one may use $l_1$ penalty? What is the difference between the two?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c5e06d",
   "metadata": {},
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6256a7",
   "metadata": {},
   "source": [
    "1. L2 imposes the penalty of limiting the variables used in training the model. so all co-efficients are shrunk by the same percentage which results in some features having near zero coeffcient and thus no say in the predictions evaluated.\n",
    "\n",
    "2. L1 regularization performs feature selection however l2 regularization decreases the weights to near zero but doesnot exactly remove them.L1 regularization may result in sparse models where as l2 regularization models are not sparse. L1 regularization is more robust than L2 regularization as it does not consider outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "36086afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logitmodel(X, y):\n",
    "    \n",
    "    # Create the logitmodel_stroke model\n",
    "    # Your code goes here\n",
    "    logitmodel_stroke = None\n",
    "    \n",
    "    # all parameters not specified are set to their defaults\n",
    "    logitmodel_stroke= linear_model.LogisticRegression(random_state=0, multi_class='multinomial', penalty='none', solver='newton-cg')\n",
    "  \n",
    "    #linmodel_realest.fit(X, y)\n",
    "    logitmodel_stroke.fit(X, y)\n",
    "    \n",
    "    #predictions = logitmodel_stroke.predict(y)\n",
    "    \n",
    "    # Train the logitmodel_stroke model\n",
    "    # Your code goes here\n",
    "\n",
    "    return logitmodel_stroke\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0cdb2",
   "metadata": {},
   "source": [
    "### Task 5 - Model evaluation (15 marks)\n",
    "The process for evaluating a classification model is different from a regression model. In regression we have a wide range of values so we measure variance, however classification has a much smaller problem space so we measure how often the correct prediction is made. There are multiple metrics for measuring this, [this article](https://www.mage.ai/blog/definitive-guide-to-accuracy-precision-recall-for-product-developers) and the [Wikipedia page](https://en.wikipedia.org/wiki/Precision_and_recall) provide additional context.\n",
    "\n",
    "1. As this is binary classification there are 2 classes. Class 1 indicates positive stroke risk and class 0 indicates negative stroke risk. \n",
    "2. When testing we use a separate dataset which the model was not trained on. This is essential to observe how the model performs on data it has not seen before.\n",
    "3. In the function below *X_ts* represents the data used to generate test predictions and *y_obs* represents the actual values we are trying to predict. \n",
    "4. We can evaluate a classification model by having it make a set of predictions for a test set (X_ts) and comparing these with the actual values (y_obs).\n",
    "5. Suppose *y_pred* is a predicted value when run on a sample from *X_ts*. We compare it to the corresponding observed value in *y_obs*. There are four potential outcomes from this comparison:\n",
    "\n",
    "    1. *y_pred* = 1 (positive) and *y_obs* = 1 (positive): counted as *true positive*.\n",
    "    2. *y_pred* = 1 (positive) and *y_obs* = 0 (negative): counted as *false positive*. \n",
    "    3. *y_pred* = 0 (negative) and *y_obs* = 0 (negative): counted as *true negative*. \n",
    "    4. *y_pred* = 0 (positive) and *y_obs* = 1 (negative): counted as *false negative*. \n",
    "    \n",
    "5. Count all the 4 cases for the entire sample input to the function *evaluate_logitmodel* and store them in 4 variables: *tp*, *fp*, *tn* and *fn*. For example, *tp* will give total number of true positives and *fn* the total of true negatives. \n",
    "6. The two metrics we will be using for evaluation are *accuracy* and *precision*. The formula for these is below. \n",
    "$$acc = \\frac{tp+tn}{tp+tn+fp+fn} \\quad\\text{(accuracy)}, \\quad prec = \\frac{tp}{tp + fp} \\quad\\text{(precision)}$$\n",
    "\n",
    "7. Run the model training/evaluation process for 5 different test/train split ratios (see task 3). Add a paragraph below outlining:\n",
    "    1. The results of your different test/train splits\n",
    "    2. How the different split sizes effected model evaluation\n",
    "    3. Was there a difference in accuracy/precision and if so, what could be causing this?\n",
    "    4. For a fixed train/test data evaluate the metrics on the train data (*X_train*) and test (*X_test*) seprately and record the valuse of the metrics.  \n",
    "8. This task is designed to test your understanding of model evaluation. **No built-in evaluation functions or metrics should be used**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "aed60c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                     int64\n",
      "gender                object\n",
      "age                  float64\n",
      "hypertension           int64\n",
      "heart_disease          int64\n",
      "ever_married          object\n",
      "avg_glucose_level    float64\n",
      "bmi                  float64\n",
      "smoking_status        object\n",
      "stroke                 int64\n",
      "dtype: object\n",
      "int64\n",
      "object\n",
      "float64\n",
      "int64\n",
      "int64\n",
      "object\n",
      "float64\n",
      "float64\n",
      "object\n",
      "int64\n",
      "Accuracy:  0.956268221574344\n",
      "Precision:  1.0\n",
      "Accuracy:  0.9467878001297858\n",
      "Precision:  1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9467878001297858, 1.0)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the model object is the output of the function fit_logitmodelto obtain y_pred\n",
    "def evaluate_logitmodel(model, X_ts,  y_obs):\n",
    "    predictions=logic_model.predict(X_ts)\n",
    "   \n",
    "    # Use the .predict() method of the model to generate a set of predictions for X_ts\n",
    "    # Your code goes here\n",
    "    tp=0\n",
    "    fp=0\n",
    "    tn=0\n",
    "    fn=0\n",
    "    # Determine the tp, fp, tn and fn values for the prediction set\n",
    "    # Your code goes here\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i]==1 and y_obs[i]==1:\n",
    "            tp=tp+1\n",
    "        elif predictions[i]==1 and y_obs[i]==0:\n",
    "            fp=fp+1\n",
    "        elif predictions[i]==0 and y_obs[i]==0:\n",
    "            tn=tn+1\n",
    "        elif predictions[i]==0 and y_obs[i]==1:\n",
    "            fn=fn+1\n",
    "\n",
    "    # Calculate the accuracy and precision values\n",
    "    # Your code goes here\n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    acc=(tp+tn)/float((tp+tn+fp+fn))\n",
    "    if(tp+fp==0):\n",
    "        tp=1\n",
    "        prec=tp/(tp+fp)\n",
    "    else:\n",
    "        prec=tp/(tp+fp)\n",
    "    print(\"Accuracy: \",acc)\n",
    "    print(\"Precision: \",prec)\n",
    "    \n",
    "    return acc, prec\n",
    "X_train, X_test, y_train, y_test = create_arrays()\n",
    "logic_model=fit_logitmodel(X_train, y_train)\n",
    "evaluate_logitmodel(logic_model, X_test,  y_test)\n",
    "evaluate_logitmodel(logic_model, X_train,  y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b20b303",
   "metadata": {},
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88ff5a",
   "metadata": {},
   "source": [
    "ANSWERS\n",
    "\n",
    "A)\n",
    "For a test size of 0.25 \n",
    "\n",
    "Accuracy and precision of the test data.\n",
    "\n",
    "Accuracy:  0.9521586931155193.\n",
    "Precision:  1.0.\n",
    "\n",
    "For a test size of 0.50 \n",
    "\n",
    "Accuracy and precision of the test data.\n",
    "\n",
    "Accuracy:  0.9497956800934034\n",
    "Precision:  0.5\n",
    "\n",
    "For a test size of 0.10 \n",
    "\n",
    "Accuracy and precision of the test data.\n",
    "\n",
    "Accuracy:  0.956268221574344\n",
    "Precision:  1.0\n",
    "\n",
    "B)\n",
    "The accuracy increased as the size of the test data decreased and that of train data increased. Further more the preision remained same at test data of 0.25 and 0.10 percent but dcecreased to half for an equal amount of test and train set.\n",
    "\n",
    "C)\n",
    "Keeping in mind the evaluations shown the precision of the model decreases as the traning set is significantly reducced to 50 percent so now the model predicts lesser correct predictions as the trainings set has become insufficient for good predictions.\n",
    "For a test data of 0.25 and 0.10 , the model gives and excellent precision of 1 which means that the model rarely predicts wrongly. Above this the accuracy of the model is pretty high toppng to 90 mark however the accuracy decreases as the training set decreases which means that a model trained on lesser data will give low accuracy.\n",
    "\n",
    "\n",
    "D)\n",
    "For a test size of 0.25 and the training size of 0.75:.\n",
    "\n",
    "Accuracy and precision of the test data.\n",
    "\n",
    "Accuracy:  0.9521586931155193.\n",
    "Precision:  1.0.\n",
    "\n",
    "Accuracy and precision of the train data.\n",
    "\n",
    "Accuracy:  0.9462616822429907.\n",
    "Precision:  1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8bb31c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
